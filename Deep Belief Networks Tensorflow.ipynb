{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Belief Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf #Deep learning Library\n",
    "import numpy as np #Matrix Algebra Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#Getting the MNIST data provided by Tensorflow\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#Loading in the mnist data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)\n",
    "trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images,\\\n",
    "    mnist.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricted Boltzmann Machines\n",
    "\n",
    "RBMs are the building blocks of deep belief nets. If you are not familiar with RBMs please check out my post describing how the RBM functions https://github.com/JosephGatto/Simplified-Restricted-Boltzmann-Machines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RBM(object):\n",
    "    def __init__(self, input_size, output_size, learning_rate, batch_size):\n",
    "        self.input_size = input_size #Size of the input layer\n",
    "        self.output_size = output_size #Size of the hidden layer\n",
    "        self.epochs = 5 #How many times we will update the weights \n",
    "        self.learning_rate = learning_rate #How big of a weight update we will perform \n",
    "        self.batch_size = batch_size #How many images will we \"feature engineer\" at at time \n",
    "        self.new_input_layer = None #Initalize new input layer variable for k-step contrastive divergence \n",
    "        self.new_hidden_layer = None\n",
    "        self.new_test_hidden_layer = None\n",
    "        \n",
    "        #Here we initialize the weights and biases of our RBM\n",
    "        #If you are wondering, the 0 is the mean of the distribution we are getting our random weights from. \n",
    "        #The .01 is the standard deviation.\n",
    "        self.w = np.random.normal(0,.01,[input_size,output_size]) #weights\n",
    "        self.hb = np.random.normal(0,.01,[output_size]) #hidden layer bias\n",
    "        self.vb = np.random.normal(0,.01,[input_size]) #input layer bias (sometimes called visible layer)\n",
    "        \n",
    "        \n",
    "        #Calculates the sigmoid probabilities of input * weights + bias\n",
    "        #Here we multiply the input layer by the weights and add the bias\n",
    "        #This is the phase that creates the hidden layer\n",
    "    def prob_h_given_v(self, visible, w, hb):\n",
    "        return tf.nn.sigmoid(tf.matmul(visible, w) + hb)\n",
    "        \n",
    "        #Calculates the sigmoid probabilities of input * weights + bias\n",
    "        #Here we multiply the hidden layer by the weights and add the input layer bias\n",
    "        #This is the reconstruction phase that recreates the original image from the hidden layer\n",
    "    def prob_v_given_h(self, hidden, w, vb):\n",
    "        return tf.nn.sigmoid(tf.matmul(hidden, tf.transpose(w)) + vb)\n",
    "    \n",
    "    #Returns new layer binary values\n",
    "    #This function returns a 0 or 1 based on the sign of the probabilities passed to it\n",
    "    #Our RBM will be utilizing binary features to represent the images\n",
    "    #This function just converts the features we have learned into a binary representation \n",
    "    def sample_prob(self, probs):\n",
    "        return tf.nn.relu(tf.sign(probs - tf.random_uniform(tf.shape(probs))))\n",
    "    \n",
    "    def train(self, X, teX):\n",
    "        #Initalize placeholder values for graph\n",
    "        #If this looks strange to you, then you have not used Tensorflow before\n",
    "        _w = tf.placeholder(tf.float32, shape = [self.input_size, self.output_size])\n",
    "        _vb = tf.placeholder(tf.float32, shape = [self.input_size])\n",
    "        _hb = tf.placeholder(tf.float32, shape = [self.output_size])\n",
    "        \n",
    "        \n",
    "        #initalize previous variables\n",
    "        #we will be saving the weights of the previous and current iterations\n",
    "        pre_w = np.random.normal(0,.01, size = [self.input_size,self.output_size])\n",
    "        pre_vb = np.random.normal(0, .01, size = [self.input_size])\n",
    "        pre_hb = np.random.normal(0, .01, size = [self.output_size])\n",
    "        \n",
    "        #initalize current variables\n",
    "        #we will be saving the weights of the previous and current iterations\n",
    "        cur_w = np.random.normal(0, .01, size = [self.input_size,self.output_size])\n",
    "        cur_vb = np.random.normal(0, .01, size = [self.input_size])\n",
    "        cur_hb = np.random.normal(0, .01, size = [self.output_size])\n",
    "               \n",
    "        #Plaecholder variable for input layer\n",
    "        v0 = tf.placeholder(tf.float32, shape = [None, self.input_size])\n",
    "         \n",
    "        #pass probabilities of input * w + b into sample prob to get binary values of hidden layer\n",
    "        h0 = self.sample_prob(self.prob_h_given_v(v0, _w, _hb ))\n",
    "        \n",
    "        #pass probabilities of new hidden unit * w + b into sample prob to get new reconstruction\n",
    "        v1 = self.sample_prob(self.prob_v_given_h(h0, _w, _vb))\n",
    "        \n",
    "        #Just get the probailities of the next hidden layer. We wont need the binary values. \n",
    "        #The probabilities here help calculate the gradients during back prop \n",
    "        h1 = self.prob_h_given_v(v1, _w, _hb)\n",
    "        \n",
    "        \n",
    "        #Contrastive Divergence\n",
    "        positive_grad = tf.matmul(tf.transpose(v0), h0) #input' * hidden0\n",
    "        negative_grad = tf.matmul(tf.transpose(v1), h1) #reconstruction' * hidden1\n",
    "        #(pos_grad - neg_grad) / total number of input samples \n",
    "        CD = (positive_grad - negative_grad) / tf.to_float(tf.shape(v0)[0]) \n",
    "        \n",
    "        #This is just the definition of contrastive divergence \n",
    "        update_w = _w + self.learning_rate * CD\n",
    "        update_vb = _vb + tf.reduce_mean(v0 - v1, 0)\n",
    "        update_hb = _hb + tf.reduce_mean(h0 - h1, 0)\n",
    "        \n",
    "        #MSE - This is our error function\n",
    "        err = tf.reduce_mean(tf.square(v0 - v1))\n",
    "        \n",
    "        #Will hold new visible layer.\n",
    "        errors = []\n",
    "        hidden_units = []\n",
    "        reconstruction = []\n",
    "        \n",
    "        test_hidden_units = []\n",
    "        test_reconstruction=[]\n",
    "        \n",
    "        \n",
    "        #The next four lines of code intitalize our Tensorflow graph and create mini batches\n",
    "        #The mini batch code is from cognitive class. I love the way they did this. Just giving credit! \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for epoch in range(self.epochs):\n",
    "                for start, end in zip(range(0, len(X), self.batch_size), range(self.batch_size, len(X), self.batch_size)):\n",
    "                    batch = X[start:end] #Mini batch of images taken from training data\n",
    "                    \n",
    "                    #Feed in batch, previous weights/bias, update weights and store them in current weights\n",
    "                    cur_w = sess.run(update_w, feed_dict = {v0:batch, _w:pre_w , _vb:pre_vb, _hb:pre_hb})\n",
    "                    cur_hb = sess.run(update_hb, feed_dict = {v0:batch, _w:pre_w , _vb:pre_vb, _hb:pre_hb})\n",
    "                    cur_vb = sess.run(update_vb, feed_dict = {v0:batch, _w:pre_w , _vb:pre_vb, _hb:pre_hb})\n",
    "                    \n",
    "                    #Save weights \n",
    "                    pre_w = cur_w\n",
    "                    pre_hb = cur_hb\n",
    "                    pre_vb = cur_vb\n",
    "                \n",
    "                #At the end of each iteration, the reconstructed images are stored and the error is outputted \n",
    "                reconstruction.append(sess.run(v1, feed_dict={v0: X, _w: cur_w, _vb: cur_vb, _hb: cur_hb}))        \n",
    "                print('Learning Rate: {}:  Batch Size: {}:  Hidden Layers: {}: Epoch: {}: Error: {}:'.format(self.learning_rate, self.batch_size, \n",
    "                                                                                                             self.output_size, (epoch+1),\n",
    "                                                                                                            sess.run(err, feed_dict={v0: X, _w: cur_w, _vb: cur_vb, _hb: cur_hb})))\n",
    "            \n",
    "            #Store final reconstruction in RBM object\n",
    "            self.new_input_layer = reconstruction[-1]\n",
    "            \n",
    "            #Store weights in RBM object\n",
    "            self.w = pre_w\n",
    "            self.hb = pre_hb\n",
    "            self.vb = pre_vb\n",
    "    \n",
    "    #This is used for Contrastive Divergence.\n",
    "    #This function makes the reconstruction your new input layer. \n",
    "    def rbm_output(self, X):\n",
    "        input_x = tf.constant(X)\n",
    "        _w = tf.constant(self.w)\n",
    "        _hb = tf.constant(self.hb)\n",
    "        _vb = tf.constant(self.vb)\n",
    "        \n",
    "        out = tf.nn.sigmoid(tf.matmul(input_x, _w) + _hb)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            return sess.run(out)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Belief Networks\n",
    "\n",
    "Deep belief nets are, in my opinion, pretty easy to understand if you have a solid grasp of RBMs. Remember, the RBM's goal is to reconstruct the original image with its hidden layer representation. The deep belief network is just a chain of RBMs. Once the first RBM has completed the image reconstruction process, a new RBM is created. The input layer of the new RBM is now the hidden layer representation of the previous RBM. We now, repeat this process as many times as we like to produce a DBN. DBNs can often give you higher level feature abstractions and produce very high quality features.\n",
    "\n",
    "Below is an example of how to use multiple RBMs to create a DBN. Here we will make 3 RBMs of hidden layer size [600 > 500 > 100]. Feel free to extract the new_hidden_layer from any of the RBMs to see how they perform as features in a lienar classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RBM_hidden_size = [600,500,100] #Three hidden layer sizes for our three layer DBN\n",
    "learning_rate = .01 \n",
    "input_size = trX.shape[1] #input layer size of original image data\n",
    "\n",
    "rbm_list = [] #This will hold all of the RBMs used in our DBN\n",
    "\n",
    "#Creates 3 RBMs\n",
    "for layer in RBM_hidden_size:\n",
    "    rbm_list.append(RBM(input_size, layer, learning_rate, 32))\n",
    "    input_size = layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Initalize input layer variables \n",
    "inpX = trX                \n",
    "test_inpx = teX\n",
    "\n",
    "#This loop is the DBN. Each rbm is trained here.\n",
    "#At the end of training, the hidden layer of the RBM is used as input\n",
    "#For the next layer of the DBN. \n",
    "for i,rbm in enumerate(rbm_list):\n",
    "    rbm_outputs = []\n",
    "    rbm_test_outputs = []\n",
    "    print('Input Shape: ', inpX.shape)\n",
    "    print('Layer: ',(i+1))\n",
    "\n",
    "    rbm.train(inpX, teX)\n",
    "    inpX = rbm.rbm_output(inpX)\n",
    "    test_inpx = rbm.rbm_output(test_inpx)\n",
    "    rbm_outputs.append(inpX)\n",
    "    rbm_test_outputs.append(test_inpx)\n",
    "\n",
    "    print('Output Shape: ', inpX.shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
